{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kcg-aQWsGPb3",
    "outputId": "f61a6ea5-78a0-4784-fe99-098492628943"
   },
   "outputs": [],
   "source": [
    "# !pip install splinter\n",
    "# !pip install webdriver-manager\n",
    "# reference blog: https://www.codecademy.com/resources/blog/web-scraping-python-beautiful-soup-mlb-stats/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQ0ELpcMAn9g"
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "j1DMxNJHJXTZ",
    "outputId": "47ed35c5-250e-48e4-80ef-490c5a69b915"
   },
   "outputs": [],
   "source": [
    "# Set up Splinter Executable path\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZA_NFw9bGzjp"
   },
   "outputs": [],
   "source": [
    "# Visit espn salaries page\n",
    "url = 'http://www.espn.com/nba/salaries'\n",
    "browser.visit(url)\n",
    "# Search all elements with the tag div with attribute list_text, then wait 1 second before searching components\n",
    "browser.is_element_present_by_css('div.list_text', wait_time=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eOv2Eq1IIoI"
   },
   "outputs": [],
   "source": [
    "# set up html parser\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bePqIkiRM71Q"
   },
   "outputs": [],
   "source": [
    "# create the dataframe\n",
    "header = soup.find('tr', class_='colhead')\n",
    "columns = [col.get_text() for col in header.find_all('td')]\n",
    "columns.append('season')\n",
    "salaries_df = pd.DataFrame(columns = columns)\n",
    "salaries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGe8oSi7M7UZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# max_page_num = soup.find('div', class_ = 'page-numbers').text\n",
    "# last_page = max_page_num[-2:]\n",
    "# get a list of the years from drop down menu\n",
    "menu = soup.select('option[value]')\n",
    "years = [year.text for year in menu[1:]]\n",
    "i = 0\n",
    "# while loop to keep the loop running as long as there is an attribute to click\n",
    "while True:\n",
    "    # Reinitialize soup for scrapping\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'html.parser')\n",
    "    \n",
    "    # grab all the players data (RK, NAME, TEAM, SALARY) on the current page\n",
    "    players = soup.find_all('tr', attrs={'class':re.compile('row')})\n",
    "    # finds the year that we are currently on\n",
    "    menu = soup.select('option[selected]')\n",
    "    year = [menu.text for menu in menu]\n",
    "    # looping through all of the soup players data\n",
    "    for player in players:       \n",
    "        # get info and salaries and append year\n",
    "        salary = [info.get_text() for info in player.find_all('td')]\n",
    "        salary.append(year[0])\n",
    "        print(salary)\n",
    "        \n",
    "        # create a temporary df\n",
    "        temp_df = pd.DataFrame(salary).transpose()\n",
    "        temp_df.columns = columns\n",
    "        salaries_df = pd.concat([salaries_df, temp_df], ignore_index = True)\n",
    "    try:\n",
    "        # find next button and press\n",
    "        next_button =  browser.find_by_tag('div[class = \"jcarousel-next\"]')\n",
    "        next_button.click()\n",
    "    except:\n",
    "        print(f\"Done Scrapping: {year}\")  \n",
    "        try:\n",
    "            i+=1\n",
    "            print(i)\n",
    "            element = browser.find_by_tag('select[class = \"tablesm\"]').click()\n",
    "            browser.find_by_text(years[i]).click()\n",
    "        except:\n",
    "            print(\"Done with Scrapping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check df if successfully scrapped\n",
    "salaries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quit browswer\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data into csv\n",
    "salaries_df.to_csv('1999-2022 Salaries.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "webscraping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
